{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5b79a0",
   "metadata": {},
   "source": [
    "# TD Method - Non-Episodic_MDP\n",
    " 에피소드가 끝나기 전에 벨류값을 업데이트 하고 싶다면, 종료하지 않는 MDP 사용 => Temporal Difference 사용(추축을 추축한다)\n",
    " 학습 시점에서 우세 하지만 실제 밸류 값과는 차이가 있는 V를 사용하므로 편향 가능성이 크다. \n",
    " 변동성이 비교적 적고, 분산도 적다. 분산이 클수록 학습이 힘들어지므로 TD가 MC보다 학습하기 좋다.\n",
    " MC와 TD의 중간은 없을까?\n",
    " => n스탭 리턴이 있는, n스텝 TD 존재!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce0ffc7c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Lib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c6ddeca",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Environment \n",
    "class GridWorld():\n",
    "    def __init__(self):\n",
    "        self.x=0\n",
    "        self.y=0\n",
    "        \n",
    "    def step(self,a):\n",
    "        # 액션 지정 4방향\n",
    "        if a == 0:\n",
    "            self.move_left()\n",
    "        elif a==1:\n",
    "            self.move_up()\n",
    "        elif a==2:\n",
    "            self.move_right()\n",
    "        elif a==3:\n",
    "            self.move_down()\n",
    "        \n",
    "        reward = -1\n",
    "        done = self.is_done()\n",
    "        return (self.x, self.y), reward, done\n",
    "    \n",
    "    def move_right(self):\n",
    "        self.y += 1\n",
    "        if self.y >3:\n",
    "            self.y = 3\n",
    "            \n",
    "    def move_left(self):\n",
    "        self.y -= 1\n",
    "        if self.y<0:\n",
    "            self.y = 0\n",
    "            \n",
    "    def move_up(self):\n",
    "        self.x -= 1\n",
    "        if self.x< 0:\n",
    "            self.x = 0\n",
    "            \n",
    "    def move_down(self):\n",
    "        self.x += 1\n",
    "        if self.x>3:\n",
    "            self.x = 3\n",
    "\n",
    "    def is_done(self):\n",
    "        if self.x ==3 and self.y==3:\n",
    "            return True\n",
    "        else :\n",
    "            return False\n",
    "        \n",
    "    def reset(self):\n",
    "        self.x =0\n",
    "        self.y =0\n",
    "        return (self.x,self.y)\n",
    "    \n",
    "    def get_state(self):\n",
    "        return (self.x,self.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470d9dee",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Agent set the actions\n",
    "class Agent(): # 각 class엔 무조건 init 함수가 있어야 한다.\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        coin = random.random() # cion = 0 ~ 1\n",
    "        \n",
    "        if coin<0.25:\n",
    "            action = 0\n",
    "        elif coin<0.5:\n",
    "            action = 1\n",
    "        elif coin<0.75:\n",
    "            action = 2\n",
    "        else :\n",
    "            action = 3\n",
    "            \n",
    "        return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ec276a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-58.07371718185942, -56.519711955378085, -53.40517816114341, -50.406110867011]\n",
      "[-55.83524124973283, -53.0325518110962, -48.641258283789156, -43.496110056026005]\n",
      "[-52.72805941996892, -48.74306136717321, -38.954941575598475, -28.339649903647025]\n",
      "[-50.40636451015198, -44.95821699003533, -29.515433759173362, 0]\n"
     ]
    }
   ],
   "source": [
    "# main function\n",
    "def main():\n",
    "    env = GridWorld()\n",
    "    agent = Agent()\n",
    "    data = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
    "    gamma = 1.0\n",
    "    alpha = 0.01 # learning rate: 한꺼번에 얼만큼의 값을 보고 업데이트 할 것인지\n",
    "    \n",
    "    for k in range(50000):\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            x,y = env.get_state()\n",
    "            action = agent.select_action()\n",
    "            (x_prime,y_prime), reward, done = env.step(action)\n",
    "            x_prime,y_prime = env.get_state()\n",
    "            \n",
    "            data[x][y] = data[x][y] + alpha*(reward+gamma*data[x_prime][y_prime]-data[x][y]) # value update 바로 업데이트\n",
    "            #print(data[x][y])\n",
    "        env.reset()\n",
    "        \n",
    "            \n",
    "    for row in data:\n",
    "        print(row)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bacc4f4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
