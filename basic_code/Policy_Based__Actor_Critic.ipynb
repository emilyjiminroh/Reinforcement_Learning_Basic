{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c761eaa6",
   "metadata": {},
   "source": [
    "# Policy Based Agent: Actor Critic \n",
    "### <=> 가치 기반 에이전트: 결정론적 특징, 각 상태와 액션이 가지는 가치가 결국엔 특정한 값으로 수렴\n",
    "- 정책 에이전트의 경우 환경에서 정채겡 맞춰 움직이는 에이전트가 경험을 쌓게 해서 강화해 나감\n",
    "- 네트워크 이외에도 손실 함수(목적 함수)를 정의해야 함. \n",
    "## Actor Critic: 정책 네트워크와 밸류 네트워크를 함께 학습하는 것\n",
    "- Q 액터 크리틱\n",
    "- 어드벤티지 액터 크리틱\n",
    "- TD 액터 크리틱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810e457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Lib\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53fabbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "gamma         = 0.98\n",
    "n_rollout     = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5bf4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(4,256) \n",
    "        self.fc_pi = nn.Linear(256,2)\n",
    "        self.fc_v = nn.Linear(256,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def pi(self, x, softmax_dim = 0): # 입력 x에 대한 정책 구하는 함수\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim) # softmax를 통해 확률을 구함\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x):  # 입력 x에 대한 가치 함수를 반환하는 함수\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "    \n",
    "    def put_data(self, transition): #에피소드의 파라미터들을 리스트에 추가\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self): #저장된 transition 데이터 배치 형태로 반환하는 함수\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s,a,r,s_prime,done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r/100.0])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            done_lst.append([done_mask])\n",
    "        \n",
    "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                                               torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                                               torch.tensor(done_lst, dtype=torch.float)\n",
    "        self.data = []\n",
    "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch #최종적으로 텐서로 변환해 반환\n",
    "  \n",
    "    def train_net(self): # 네트워크를 학습하는 함수\n",
    "        s, a, r, s_prime, done = self.make_batch() #저장된 transition 데이터로부터 배치를 만듦\n",
    "        td_target = r + gamma * self.v(s_prime) * done # 타깃 가치 함수값인 TD 타깃을 계산\n",
    "        delta = td_target - self.v(s) #: TD 오차인 delta를 계산\n",
    "        \n",
    "        pi = self.pi(s, softmax_dim=1) #현재 상태에 대한 정책을 계산\n",
    "        pi_a = pi.gather(1,a) #현재 상태에서 취한 행동에 대한 확률을 추출\n",
    "        loss = -torch.log(pi_a) * delta.detach() + F.smooth_l1_loss(self.v(s), td_target.detach()) #: 손실 함수를 계산\n",
    "\n",
    "        self.optimizer.zero_grad() #옵티마이저의 그래디언트를 초기화\n",
    "        loss.mean().backward() #손실의 평균을 계산하고, 그래디언트를 역전파\n",
    "        self.optimizer.step() # 옵티마이저를 사용하여 신경망의 가중치를 업데이트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8d12db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AIMLAB_JKIM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "C:\\Users\\AIMLAB_JKIM\\AppData\\Local\\Temp\\ipykernel_2932\\3860977774.py:36: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 17.6\n",
      "# of episode :40, avg score : 18.1\n",
      "# of episode :60, avg score : 19.2\n",
      "# of episode :80, avg score : 16.1\n",
      "# of episode :100, avg score : 20.4\n",
      "# of episode :120, avg score : 17.2\n",
      "# of episode :140, avg score : 18.6\n",
      "# of episode :160, avg score : 17.8\n",
      "# of episode :180, avg score : 17.3\n",
      "# of episode :200, avg score : 16.9\n",
      "# of episode :220, avg score : 19.6\n",
      "# of episode :240, avg score : 18.6\n",
      "# of episode :260, avg score : 23.0\n",
      "# of episode :280, avg score : 23.8\n",
      "# of episode :300, avg score : 21.4\n",
      "# of episode :320, avg score : 28.4\n",
      "# of episode :340, avg score : 34.0\n",
      "# of episode :360, avg score : 33.0\n",
      "# of episode :380, avg score : 32.3\n",
      "# of episode :400, avg score : 33.6\n",
      "# of episode :420, avg score : 36.0\n",
      "# of episode :440, avg score : 37.2\n",
      "# of episode :460, avg score : 33.5\n",
      "# of episode :480, avg score : 34.0\n",
      "# of episode :500, avg score : 36.4\n",
      "# of episode :520, avg score : 56.1\n",
      "# of episode :540, avg score : 52.1\n",
      "# of episode :560, avg score : 64.5\n",
      "# of episode :580, avg score : 50.1\n",
      "# of episode :600, avg score : 47.9\n",
      "# of episode :620, avg score : 66.1\n",
      "# of episode :640, avg score : 57.2\n",
      "# of episode :660, avg score : 98.2\n",
      "# of episode :680, avg score : 98.2\n",
      "# of episode :700, avg score : 75.5\n",
      "# of episode :720, avg score : 90.8\n",
      "# of episode :740, avg score : 165.2\n",
      "# of episode :760, avg score : 146.7\n",
      "# of episode :780, avg score : 174.5\n",
      "# of episode :800, avg score : 146.8\n",
      "# of episode :820, avg score : 119.0\n",
      "# of episode :840, avg score : 236.6\n",
      "# of episode :860, avg score : 253.9\n",
      "# of episode :880, avg score : 288.6\n",
      "# of episode :900, avg score : 144.4\n",
      "# of episode :920, avg score : 132.4\n",
      "# of episode :940, avg score : 150.8\n",
      "# of episode :960, avg score : 160.9\n",
      "# of episode :980, avg score : 149.6\n"
     ]
    }
   ],
   "source": [
    "def main():  \n",
    "    env = gym.make('CartPole-v1')\n",
    "    model = ActorCritic()    \n",
    "    print_interval = 20\n",
    "    score = 0.0\n",
    "\n",
    "    for n_epi in range(1000): \n",
    "        done = False\n",
    "        s, _ = env.reset()\n",
    "        while not done:\n",
    "            for t in range(n_rollout):# 에이전트의 액션 선택\n",
    "                prob = model.pi(torch.from_numpy(s).float()) #현재 상태를 모델에 입력해 정책을 구함\n",
    "                m = Categorical(prob) #확률 분포를 기반으로 범주형 분포 객체를 생성\n",
    "                a = m.sample().item() #범주형 분포로부터 행동을 샘플링하여 선택\n",
    "                s_prime, r, done, truncated, info = env.step(a) #택한 행동을 환경에 적용하여 다음 상태와 보상 등을 얻음\n",
    "                model.put_data((s,a,r,s_prime,done)) # 데이터를 모델의 버퍼에 저장\n",
    "                \n",
    "                s = s_prime # 다음상태를 현재 상태로 업데이트\n",
    "                score += r #누적보상 계산\n",
    "                \n",
    "                if done:\n",
    "                    break                     \n",
    "            \n",
    "            model.train_net() # 모댈 학습\n",
    "            \n",
    "        if n_epi%print_interval==0 and n_epi!=0:# 정해진 간격마다 학습결과값 출력\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0 \n",
    "    env.close() # 환경 닫음\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
